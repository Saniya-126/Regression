{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "* Simple linear regression is a statistical method that models the relationship between two variables by fitting a linear equation to observed data.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "* The key assumptions of simple linear regression are linearity, independence, homoscedasticity, normality of residuals, and no multicollinearity.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "* In the equation  Y = mX + c , the coefficient  m  represents the slope of the line, indicating the change in the dependent variable Y  for a one-unit increase in the independent variable  X .\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "* In the equation  Y = mX + c , the intercept c  represents the value of the dependent variable Y  when the independent variable X  is equal to zero.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "* The slope \\( m \\) in simple linear regression is calculated using the formula:\n",
        "\n",
        "m = {n∑xy−(∑x)(∑y)}/{n∑x^2) - (n∑x)^2}\n",
        "\n",
        "where:\n",
        "- N is the number of data points,\n",
        "- ∑xy  is the sum of the product of each pair of  X  and  Y  values,\n",
        "- ∑x  is the sum of the  X values,\n",
        "- ∑y is the sum of the  Y  values,\n",
        "- ∑x^2  is the sum of the squares of the  X  values.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "*The purpose of the least squares method in simple linear regression is to minimize the sum of the squared differences (residuals) between the observed values and the values predicted by the linear model, thereby finding the best-fitting line that represents the relationship between the independent and dependent variables.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "* The coefficient of determination (R²) in simple linear regression is interpreted as the proportion of the variance in the dependent variable that can be explained by the independent variable; it ranges from 0 to 1, where 0 indicates that the independent variable does not explain any variance in the dependent variable, and 1 indicates that it explains all the variance.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "* Multiple linear regression is a statistical technique that models the relationship between a dependent variable and two or more independent variables by fitting a linear equation to the observed data.\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "* The main difference between simple and multiple linear regression is that simple linear regression involves one independent variable to predict a dependent variable, while multiple linear regression involves two or more independent variables to predict the same dependent variable.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "* The main difference between simple and multiple linear regression is that simple linear regression involves one independent variable to predict a dependent variable, while multiple linear regression involves two or more independent variables to predict the same dependent variable.\n",
        "\n",
        "11.  What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "* Heteroscedasticity refers to the condition in which the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variables. It can affect the results of a multiple linear regression model by leading to inefficient estimates, biased standard errors, and invalid hypothesis tests, which can ultimately result in unreliable confidence intervals and significance tests for the regression coefficients.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "* To improve a multiple linear regression model with high multicollinearity, you can remove or combine highly correlated predictors, use regularization techniques like ridge or lasso regression, or apply principal component analysis (PCA) to create uncorrelated predictors. Increasing the sample size can also help distinguish the effects of correlated variables. These strategies enhance model stability and interpretability.\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "* Common techniques for transforming categorical variables for use in regression models include one-hot encoding, which creates binary columns for each category, and label encoding, which assigns a unique integer to each category. These transformations allow categorical data to be effectively incorporated into regression analyses.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "* Interaction terms in multiple linear regression are used to assess whether the effect of one independent variable on the dependent variable changes depending on the level of another independent variable. By including interaction terms, the model can capture more complex relationships and provide a better understanding of how variables work together to influence the outcome.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "* In simple linear regression, the intercept represents the expected value of the dependent variable when the independent variable is zero. In multiple linear regression, the intercept represents the expected value of the dependent variable when all independent variables are set to zero, which may not always be meaningful if zero is outside the range of the data or if it does not make sense in the context of the variables involved.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "* The slope in regression analysis indicates the expected change in the dependent variable for a one-unit increase in the independent variable, reflecting the strength and direction of their relationship. A steeper slope suggests a stronger effect on predictions, while a slope of zero indicates no relationship, leading to constant predictions regardless of the independent variable's value.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "* The intercept in a regression model provides a baseline value of the dependent variable when all independent variables are set to zero, offering context for the starting point of the relationship. It helps interpret the model by indicating the expected outcome in scenarios where the independent variables have no influence, although its practical significance may vary depending on the data context.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "* Using R² as a sole measure of model performance is limited because it does not account for the complexity of the model, can be misleading in the presence of multicollinearity, and does not indicate whether the model is appropriate or predictive for new data.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "* A large standard error for a regression coefficient indicates that there is considerable variability in the estimate, suggesting that the coefficient may not be statistically significant and that the relationship between the independent and dependent variables may be uncertain.\n",
        "\n",
        "20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "* Heteroscedasticity can be identified in residual plots by observing a pattern where the spread of residuals increases or decreases with the fitted values, indicating non-constant variance; addressing it is important because it can lead to inefficient estimates and invalid statistical inferences.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "* A high R² coupled with a low adjusted R² suggests that the model may be overfitting the data by including too many predictors that do not contribute meaningfully to explaining the variance in the dependent variable, leading to a lack of generalizability.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "* Scaling variables in multiple linear regression is important because it ensures that all predictors contribute equally to the model, improves numerical stability, and enhances the interpretability of coefficients, especially when variables are on different scales.\n",
        "\n",
        "23. What is polynomial regression?\n",
        "* Polynomial regression is a form of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial, allowing for the modeling of non-linear relationships.\n",
        "\n",
        "24. How does polynomial regression differ from linear regression?\n",
        "* Polynomial regression differs from linear regression in that it can model non-linear relationships by including polynomial terms of the independent variable, whereas linear regression assumes a straight-line relationship.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "* Polynomial regression is used when the relationship between the independent and dependent variables is non-linear, allowing for a more flexible fit to the data by capturing curves and trends that a simple linear model cannot.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "* The general equation for polynomial regression of degree ( n ) is\n",
        "     Y = β0 + β1X + β2X2 + … + βhXh + ε\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "* Yes, polynomial regression can be applied to multiple variables by including polynomial terms for each independent variable, allowing for complex interactions and non-linear relationships among them.\n",
        "\n",
        "28.  What are the limitations of polynomial regression?\n",
        "* The limitations of polynomial regression include the risk of overfitting with high-degree polynomials, increased model complexity, sensitivity to outliers, and difficulty in interpreting coefficients as the degree increases.\n",
        "\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "* Methods to evaluate model fit when selecting the degree of a polynomial include cross-validation, examining the adjusted R², analyzing residual plots, and using information criteria such as AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion).\n",
        "\n",
        "30.  Why is visualization important in polynomial regression?\n",
        "* Visualization is important in polynomial regression because it helps to intuitively assess the fit of the model, identify patterns, detect potential overfitting, and understand the relationship between variables more clearly.\n",
        "\n",
        "31.  How is polynomial regression implemented in Python?\n",
        "* Polynomial regression in Python can be implemented using libraries like `numpy` for creating polynomial features and `scikit-learn` for fitting the regression model, typically with `PolynomialFeatures` and `LinearRegression`.\n",
        "\n"
      ],
      "metadata": {
        "id": "oK4Y1Qd-mHhC"
      }
    }
  ]
}